{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PrepareInputOutputData:\n",
    "    processed_image_folder : Path\n",
    "    blur_image_folder : Path\n",
    "\n",
    "@dataclass\n",
    "class ModelParametersConfig:\n",
    "    input_shape : tuple\n",
    "    batch_size : int\n",
    "    kernel_size : int\n",
    "    latent_dim : int\n",
    "    layer_filters : list\n",
    "\n",
    "@dataclass\n",
    "class TrainingModelConfig:\n",
    "    model_dir : Path\n",
    "    HDFmodel_path : Path\n",
    "    model_path : Path\n",
    "    loss:str\n",
    "    optimizer: str\n",
    "    metrics:list\n",
    "    batch_size: int\n",
    "    epochs:int\n",
    "    model_history_path :Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder.constants import filepath\n",
    "from autoencoder import logger\n",
    "from autoencoder.utils.util_functions import create_dir, read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager():\n",
    "    def __init__(self,\n",
    "                 config_filepath = filepath.CONFIG_FILE_PATH,\n",
    "                 params_filepath = filepath.PARAMS_FILE_PATH,\n",
    "                 secret_filepath = filepath.SECRET_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.secret = read_yaml(secret_filepath)\n",
    "    \n",
    "    def get_data_preparation_config(self) -> PrepareInputOutputData:\n",
    "        config = self.config.data_paths\n",
    "\n",
    "        input_output_data_config = PrepareInputOutputData(\n",
    "            processed_image_folder=config.processed_image_folder,\n",
    "            blur_image_folder= config.blur_image_folder\n",
    "        )\n",
    "\n",
    "        return input_output_data_config\n",
    "    \n",
    "    def get_model_param_config(self) -> ModelParametersConfig:\n",
    "        parameters = self.params\n",
    "\n",
    "        model_param_config = ModelParametersConfig(\n",
    "            input_shape=tuple(parameters.input_shape),\n",
    "            batch_size= parameters.batch_size,\n",
    "            kernel_size= parameters.kernel_size,\n",
    "            latent_dim=parameters.latent_dim,\n",
    "            layer_filters=  parameters.layer_filters\n",
    "        )\n",
    "\n",
    "        return model_param_config\n",
    "    \n",
    "    def get_model_training_config(self) -> TrainingModelConfig:\n",
    "        config = self.config.model_paths\n",
    "        params = self.params\n",
    "\n",
    "        model_training_config = TrainingModelConfig(\n",
    "            model_dir= config.model_dir,\n",
    "            HDFmodel_path=config.HDFmodel_path,\n",
    "            model_path=config.model_path,\n",
    "            loss=params.loss,\n",
    "            optimizer=params.optimizer,\n",
    "            metrics=params.metrics,\n",
    "            batch_size=params.batch_size,\n",
    "            epochs=params.epochs,\n",
    "            model_history_path=config.model_history_path\n",
    "\n",
    "        )\n",
    "\n",
    "        return model_training_config\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Model Training\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import Model, Input, regularizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers import Reshape, Conv2DTranspose, BatchNormalization, Add\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilding():\n",
    "    def __init__(self, params_config = ModelParametersConfig):\n",
    "        self.params_config = params_config\n",
    "\n",
    "    def residual_block(self, x, filters, kernel_size, stride=1):\n",
    "        shortcut = x\n",
    "        x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.add([shortcut, x])\n",
    "        x = layers.ReLU()(x)\n",
    "        return x\n",
    "    \n",
    "    def build_autoencoder(self) -> tf.keras.Model:\n",
    "        input_shape = self.params_config.input_shape\n",
    "        kernel_size = self.params_config.kernel_size\n",
    "        latent_dim = self.params_config.latent_dim\n",
    "        layer_filters = self.params_config.layer_filters\n",
    "        logger.info(f'The Input shape is {input_shape}')\n",
    "        logger.info(f'The kernel size is {kernel_size} ')\n",
    "        logger.info(f'The latent vector Lenght is {latent_dim}')\n",
    "        logger.info(\"Building Encoder Part of the Model\")\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = layers.Conv2D(layer_filters[0], (kernel_size, kernel_size), padding='same', strides=2)(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "        x = self.residual_block(x, layer_filters[0], kernel_size)\n",
    "        x = layers.Conv2D(layer_filters[1], (kernel_size, kernel_size), padding='same', strides=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "        x = self.residual_block(x, layer_filters[1], kernel_size)\n",
    "        x = layers.Conv2D(layer_filters[2], (kernel_size, kernel_size), padding='same', strides=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "        x = self.residual_block(x, layer_filters[2], kernel_size)\n",
    "\n",
    "        # Flatten and Latent Vector\n",
    "        x = layers.Flatten()(x)\n",
    "        latent = layers.Dense(latent_dim, name='latent_vector')(x)\n",
    "\n",
    "        # Decoder\n",
    "        logger.info(\"Building Dencoder Part of the Model\")\n",
    "        x = layers.Dense(16 * 16 * layer_filters[2])(latent)  # Adjust dimensions to match encoder output shape\n",
    "        x = layers.Reshape((16, 16, layer_filters[2]))(x)\n",
    "        \n",
    "        x = self.residual_block(x, layer_filters[2], kernel_size)\n",
    "        x = layers.Conv2DTranspose(layer_filters[1], (kernel_size, kernel_size), padding='same', strides=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "        x = self.residual_block(x, layer_filters[1], kernel_size)\n",
    "        x = layers.Conv2DTranspose(layer_filters[0], (kernel_size, kernel_size), padding='same', strides=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "        x = self.residual_block(x, layer_filters[0], kernel_size)\n",
    "        x = layers.Conv2DTranspose(32, (kernel_size, kernel_size), padding='same', strides=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "        x = self.residual_block(x, 32, kernel_size)\n",
    "\n",
    "        # Output layer\n",
    "        outputs = layers.Conv2D(3, (kernel_size, kernel_size), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "        # Autoencoder model\n",
    "        autoencoder = models.Model(inputs, outputs)\n",
    "        logger.info(\"Autoencoder Model Building Process Completed\")\n",
    "        return autoencoder\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataForTraining():\n",
    "    def __init__(self, model_config:PrepareInputOutputData):\n",
    "        self.model_config = model_config\n",
    "\n",
    "    def image_to_array(self, folder_path) -> np.ndarray:\n",
    "        array = []\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            try:\n",
    "                img = image.load_img(img_path)\n",
    "                img = image.img_to_array(img)\n",
    "                img = img/255.0\n",
    "                array.append(img)\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        logger.info(\"Image Data Successfully Converted into numpy array\")\n",
    "        return np.array(array)\n",
    "    \n",
    "    def prepareinputdata(self)->np.ndarray:\n",
    "        try:\n",
    "            logger.info(\"Converting Blur Image Data into Array for Model Input\")\n",
    "            input_data_path = self.model_config.blur_image_folder\n",
    "            input_data = self.image_to_array(input_data_path)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        return input_data\n",
    "\n",
    "    def prepareoutputdata(self)->np.ndarray:\n",
    "        try:\n",
    "            logger.info(\"Converting Clean Image Data into Array for Model Output\")\n",
    "            output_data_path = self.model_config.blur_image_folder\n",
    "            output_data = self.image_to_array(output_data_path)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder.utils.util_functions import create_dir\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel():\n",
    "    def __init__(self, train_config = TrainingModelConfig):\n",
    "        self.train_config = train_config\n",
    "\n",
    "    def train_model(self, x : np.ndarray, y : np.ndarray, model : tf.keras.Model):\n",
    "        logger.info(f\"Compiling Autoencoder Model with loss : {self.train_config.loss}\")\n",
    "        logger.info(f\"Compiling Autoencoder Model with optimizer : {self.train_config.optimizer}\")\n",
    "        logger.info(f\"Compiling Autoencoder Model with metrics : {self.train_config.metrics}\")\n",
    "        model.compile(loss=self.train_config.loss, optimizer = self.train_config.optimizer, metrics=self.train_config.metrics)\n",
    "        logger.info(\"Model Compiled Successfully\")\n",
    "        lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               verbose=1,\n",
    "                               min_lr=0.5e-6)\n",
    "        callbacks = [lr_reducer]\n",
    "        logger.info(f'Training Model for Epoch : {self.train_config.epochs}')\n",
    "        logger.info(f'Training Model with BatchSize : {self.train_config.batch_size}')\n",
    "        history = model.fit(x, y, epochs=self.train_config.epochs,batch_size=self.train_config.batch_size, callbacks = callbacks)\n",
    "        logger.info('Model Trained Successfully Now saving model') \n",
    "        create_dir([self.train_config.model_dir])\n",
    "        model.export(self.train_config.model_path)\n",
    "        model.save(self.train_config.HDFmodel_path)\n",
    "        with open(self.train_config.model_history_path, 'w') as f:\n",
    "            json.dump(history.history, f)\n",
    "        logger.info(f'model saved at {self.train_config.model_path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-24 14:39:33,021:INFO:util_functions:yaml file: ..\\config\\config.yaml loaded successfully]\n",
      "[2024-09-24 14:39:33,026:INFO:util_functions:yaml file: ..\\params.yaml loaded successfully]\n",
      "[2024-09-24 14:39:33,028:INFO:util_functions:yaml file: ..\\secret\\secrets.yaml loaded successfully]\n",
      "[2024-09-24 14:39:33,030:INFO:3125536882:Converting Blur Image Data into Array for Model Input]\n",
      "[2024-09-24 14:39:33,034:INFO:3125536882:Image Data Successfully Converted into numpy array]\n",
      "[2024-09-24 14:39:33,035:INFO:3125536882:Converting Clean Image Data into Array for Model Output]\n",
      "[2024-09-24 14:39:33,038:INFO:3125536882:Image Data Successfully Converted into numpy array]\n",
      "[2024-09-24 14:39:33,039:INFO:334777223:The Input shape is (128, 128, 3)]\n",
      "[2024-09-24 14:39:33,040:INFO:334777223:The kernel size is 3 ]\n",
      "[2024-09-24 14:39:33,040:INFO:334777223:The latent vector Lenght is 256]\n",
      "[2024-09-24 14:39:33,043:INFO:334777223:Building Encoder Part of the Model]\n",
      "[2024-09-24 14:39:33,225:INFO:334777223:Building Dencoder Part of the Model]\n",
      "[2024-09-24 14:39:33,441:INFO:334777223:Autoencoder Model Building Process Completed]\n",
      "[2024-09-24 14:39:33,456:INFO:655803565:Compiling Autoencoder Model with loss : mse]\n",
      "[2024-09-24 14:39:33,457:INFO:655803565:Compiling Autoencoder Model with optimizer : adam]\n",
      "[2024-09-24 14:39:33,457:INFO:655803565:Compiling Autoencoder Model with metrics : ['acc']]\n",
      "[2024-09-24 14:39:33,457:INFO:655803565:Model Compiled Successfully]\n",
      "[2024-09-24 14:39:33,457:INFO:655803565:Training Model for Epoch : 10]\n",
      "[2024-09-24 14:39:33,457:INFO:655803565:Training Model with BatchSize : 32]\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step - acc: 0.1121 - loss: 0.1201 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 734ms/step - acc: 0.1615 - loss: 0.0950 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 741ms/step - acc: 0.1834 - loss: 0.0581 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750ms/step - acc: 0.2471 - loss: 0.0430 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 769ms/step - acc: 0.4464 - loss: 0.0330 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 743ms/step - acc: 0.7024 - loss: 0.0276 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step - acc: 0.7178 - loss: 0.0250 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step - acc: 0.7313 - loss: 0.0241 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step - acc: 0.7472 - loss: 0.0237 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step - acc: 0.7672 - loss: 0.0234 - learning_rate: 0.0010\n",
      "[2024-09-24 14:39:53,833:INFO:655803565:Model Trained Successfully Now saving model]\n",
      "[2024-09-24 14:39:53,839:INFO:util_functions:Created directory at : ../artifacts/models]\n",
      "[2024-09-24 14:39:54,759:INFO:signature_serialization:Function `__call__` contains input name(s) resource with unsupported characters which will be renamed to functional_1_1_conv2d_35_1_reshape_readvariableop_resource in the SavedModel.]\n",
      "[2024-09-24 14:39:54,971:INFO:signature_serialization:Function `__call__` contains input name(s) resource with unsupported characters which will be renamed to functional_1_1_conv2d_35_1_reshape_readvariableop_resource in the SavedModel.]\n",
      "[2024-09-24 14:40:01,682:INFO:builder_impl:Assets written to: ../artifacts/models/model\\assets]\n",
      "Saved artifact at '../artifacts/models/model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor_73')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2113759777888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759778944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759693984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759693632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759778064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759694160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759691872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759692224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759672624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759671568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759691696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113759674032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761084864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761084512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761097152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761098208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761095744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761096976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761107104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761106928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761124416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761124592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761117984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761119216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761128112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761127936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761153616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761153792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761138992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761140224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761156960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761173744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761177840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761178896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761176384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761177664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761204176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761204000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761212368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761213424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761210960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761212192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761233376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761233200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761253856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761254912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761252448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761253680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761266672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761266496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761295344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761316928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761293936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761295168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761351456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761351104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761359472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761359120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761388144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761387792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761400432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761401488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761399024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761400256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761421264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761421088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761433552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761434608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761432144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761433376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761447600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761447424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761489488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761489664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761466672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761467904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761492832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761505520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761530096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761531152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761508160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761529920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761551104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761550752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761567488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761568544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761553744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761567312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761610208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761610032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761618400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761619456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761616992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761618224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761631216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761631040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761655792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761677376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761654384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761655616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761680896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761680720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761718688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761718864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761695872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761697104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761748240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761747888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761768720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761769776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761750880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761768544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761797920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761797568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761802016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761803072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761800608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761801840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761831216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761831040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761843504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761844560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761842096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761843328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761869488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2113761900672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "[2024-09-24 14:40:02,049:WARNING:saving_api:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n",
      "[2024-09-24 14:40:04,875:INFO:655803565:model saved at ../artifacts/models/model]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    data_preparation_config = config_manager.get_data_preparation_config()\n",
    "    preparedata = PrepareDataForTraining(data_preparation_config)\n",
    "    input_data = preparedata.prepareinputdata()\n",
    "    output_data = preparedata.prepareoutputdata()\n",
    "\n",
    "    model_param_config = config_manager.get_model_param_config()\n",
    "    building_model = ModelBuilding(model_param_config)\n",
    "    autoencoder = building_model.build_autoencoder()\n",
    "\n",
    "    model_train_config = config_manager.get_model_training_config()\n",
    "    training_model = TrainModel(model_train_config)\n",
    "    training_model.train_model(input_data, output_data, autoencoder)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
